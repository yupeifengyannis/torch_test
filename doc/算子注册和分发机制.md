看pytorch的一些疑惑点

１、是什么是boxed api（貌似和jit有关系）

２、什么是unboxed api（貌似和eager有关系）

aten代码自动生成：

在build/aten/src/ATen目录下面是自动生成的代码，这些文件基本都是以aten/src/ATen/templates/为模板来在代码编译前自动生成的。所以在代码编译的时候会讲build/aten/src/ATen目录下面的代码给编译进去的。

简单看一下自动生成代码的目录结构

第一部分：

```
build/aten/src/ATen/BackendSelectRegister.cpp
build/aten/src/ATen/CPUType.cpp
build/aten/src/ATen/CPUType.h
build/aten/src/ATen/Declarations.yaml
build/aten/src/ATen/Functions.cpp
build/aten/src/ATen/Functions.h
build/aten/src/ATen/LegacyTHFunctionsCPU.cpp
build/aten/src/ATen/LegacyTHFunctionsCPU.h
build/aten/src/ATen/MkldnnCPUType.cpp
build/aten/src/ATen/MkldnnCPUType.h
build/aten/src/ATen/NativeFunctions.h
build/aten/src/ATen/QuantizedCPUType.cpp
build/aten/src/ATen/QuantizedCPUType.h
build/aten/src/ATen/SparseCPUType.cpp
build/aten/src/ATen/SparseCPUType.h
build/aten/src/ATen/TypeDefault.cpp
build/aten/src/ATen/TypeDefault.h
```

第二部分：

```
build/aten/src/ATen/CUDAType.cpp
build/aten/src/ATen/CUDAType.h
build/aten/src/ATen/LegacyTHFunctionsCUDA.cpp
build/aten/src/ATen/LegacyTHFunctionsCUDA.h
build/aten/src/ATen/QuantizedCUDAType.cpp
build/aten/src/ATen/QuantizedCUDAType.h
build/aten/src/ATen/SparseCUDAType.cpp
build/aten/src/ATen/SparseCUDAType.h
```

第三部分：

```
build/aten/src/ATen/core/ATenOpList.cpp
build/aten/src/ATen/core/TensorBody.h
build/aten/src/ATen/core/TensorMethods.cpp
```

第四部分：

build/aten/src/ATen/native，这个目录里面存放的是cpu的kernel函数，我们可以对比一下build/aten/src/ATen/native/cpu目录和aten/src/ATen/native/cpu目录下里面函数的区别，其实build/aten/src/ATen/native/cpu就是将aten/src/ATen/native/cpu里面的文件拷贝过来，然后复制变成３份，DEFAULT，AVX和AVX2。

![image-20210417093957882](/home/yupeifeng/.config/Typora/typora-user-images/image-20210417093957882.png)

这里需要注意一个问题，其实aten/src/ATen/native/cpu部分的代码实际上是没有编译到动态库中的，实际编译到动态库中的代码都是build/aten/src/ATen/native/cpu目录下的文件。在编译系统中会将AVX、AVX2和DEFAULT都编译一次。

我们写一个简单的调用torch.add的算子

peifeng_test/python/debug_add.py

```python
import torch
input1 = torch.rand(2,2)
input2 = torch.rand(2,2)
output = torch.add(input1, input2)
```

前提在于我们的pytorch要编译debug版本，可以pytorch的编译一小节，然后使用gdb进行打断点调试。执行以下三步命令：

１、gdb python，这个是将python加载到gdb中。

２、b build/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp:24　打上断点

３、run peifeng_test/python/debug_add.py运行我们之前的脚本，我们可以看到程序会在断点处停下来，因为我的CPU是带有AVX2向量计算的能力的。

```bash
(base) yupeifeng@yupeifeng-fish:~/Documents/github/pytorch$ gdb python
GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1
Copyright (C) 2016 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
<http://www.gnu.org/software/gdb/documentation/>.
For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from python...done.
(gdb) 
(gdb) b build/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp:24                                                              
No source file named build/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp.
Make breakpoint pending on future shared library load? (y or [n]) y
Breakpoint 1 (build/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp:24) pending.
(gdb) run peifeng_test/python/debug_add.py
Starting program: /home/yupeifeng/miniconda3/bin/python peifeng_test/python/debug_add.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

Breakpoint 1, at::native::(anonymous namespace)::add_kernel (iter=..., alpha_scalar=...)
    at aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp:24
24      void add_kernel(TensorIterator& iter, Scalar alpha_scalar) {
(gdb) p alpha_scalar
$1 = {tag = c10::Scalar::Tag::HAS_i, v = {d = 4.9406564584124654e-324, i = 1, z = {real_ = 4.9406564584124654e-324, 
      imag_ = 4.6355715027768967e-310}}}
(gdb) 
```

算子的分配注册机制，这里需要看一些文件

１、aten/src/ATen/native/DispatchStub.h

２、aten/src/ATen/native/DispatchStub.cpp

首先可以看aten/src/ATen/native/DispatchStub.h文件中的说明，意思对于每种算子我们都需要定义一个它自己的DispatchStub类，然后这个算子不是有一大堆不同的kernel实现吗，我们在对应的kernel中调用REGISTER_DISPATCH将kernel函数指针注册给DispatchStub类。

```
// Implements instruction set specific function dispatch.
//
// Kernels that may make use of specialized instruction sets (e.g. AVX) are
// compiled multiple times with different compiler flags (e.g. -mavx). A
// DispatchStub contains a table of function pointers for a kernel. At runtime,
// the fastest available kernel is chosen based on the features reported by
// cpuinfo.
//
// Example:
//
// In native/MyKernel.h:
//   using fn_type = void(*)(const Tensor& x);
//   DECLARE_DISPATCH(fn_type, stub);
//
// In native/MyKernel.cpp
//   DEFINE_DISPATCH(stub);
//
// In native/cpu/MyKernel.cpp:
//   namespace {
//     // use anonymous namespace so that different cpu versions won't conflict
//     void kernel(const Tensor& x) { ... }
//   }
//   REGISTER_DISPATCH(stub, &kernel);
//
// To call:
//   stub(kCPU, tensor);
//
```

我们以add算子为例子来看一下声明、定义和注册kernel的内容。

## 声明

aten/src/ATen/native/BinaryOps.h

```c++
DECLARE_DISPATCH(binary_fn_alpha, add_stub);
```

这个宏定义是在aten/src/ATen/native/DispatchStub.h文件中，

```c++
#define DECLARE_DISPATCH(fn, name)         \
  struct name : DispatchStub<fn, name> {   \
    name() = default;                      \
    name(const name&) = delete;            \
    name& operator=(const name&) = delete; \
  };                                       \
  extern CAFFE2_API struct name name
```

那么上述的表达式一展开就是线面这个结构体的声明式。

首先binary_fn_alpha是个啥呢？

看着这个binary_fn_alpha就是一个函数指针类型。

```
using binary_fn_alpha = void(*)(TensorIterator&, Scalar alpha);
```



```c++
struct add_stub : DispatchStub<binary_fn_alpha, add_stub>{
	add_stub() = default;
	add_stub(const add_stub&) = delete;
	add_stub＆ operator=(const add_stub&) = delete;
};
extern struct add_stub add_stub;
```

然后在简单看看DisptachStub<binary_fn_alpha, add_stub>具体回事什么个样子的代码。







## 定义

aten/src/ATen/native/BinaryOps.cpp

```c++
DEFINE_DISPATCH(add_stub);
```

这个很简单展开就是定义一下add_stub

```c++
struct add_stub add_stub;
```

## 注册kernel

在build/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp文件中写了一个add_kernel的函数，然后我们将这个函数add_kernel的函数指针给放到add_stub中。

首先看看add_kernel是啥样的

```
void add_kernel(TensorIterator& iter, Scalar alpha_scalar) {
  if (iter.dtype() == ScalarType::Bool) {
      using scalar_t = bool;
      auto alpha = alpha_scalar.to<scalar_t>();
      cpu_kernel(iter,
        [=](scalar_t a, scalar_t b) __ubsan_ignore_undefined__ -> scalar_t { return a + alpha * b; });
  } else {
    AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kBFloat16, kHalf, iter.dtype(), "add_cpu/sub_cpu", [&]() {
      auto alpha = alpha_scalar.to<scalar_t>();
      auto alpha_vec = Vec256<scalar_t>(alpha);
      cpu_kernel_vec(iter,
        [=](scalar_t a, scalar_t b) __ubsan_ignore_undefined__ -> scalar_t { return a + alpha * b; },
        [=](Vec256<scalar_t> a, Vec256<scalar_t> b) __ubsan_ignore_undefined__ {
          return vec256::fmadd(b, alpha_vec, a);
        });
      });
  }
}
```

暂时可以先不管算子内部具体是怎么实现的，我们只了解一下接口和函数类型，add_kernel的函数类型就是void(*)(TensorIterator&, Scalar alpha)这种类型的。

```c++
REGISTER_DISPATCH(add_stub, &add_kernel);
```

我们简单的看一下他的调用流程

```
REGISTER_DISPATCH(add_stub, &add_kernel);
// 上面这个宏定义展开其实就是：
// aten/src/ATen/native/DispatchStub.h
REGISTER_ARCH_DISPATCH(add_stub, CPU_CAPABILITY, &add_kernel)
// 然后接着展开
REGISTER_ARCH_DISPATCH(add_stub, CPU_CAPABILITY, &add_kernel)
// 其实这里的CPU_CAPABILITY就是AVX2
template<>
decltype(&add_kernel) DispatchStub<decltype(&add_kernel), struct add_stub>::AVX2＝＆add_kernel；
//这句话看着莫名其妙的，其实就是对DistpathStub类中的静态成员AVX2进行初始化操作。
// AVX2函数指针和&add_kernel函数指针的类型都是一样的，都是void(*)(TensorIterator&, Scalar alpha)这种类型，所以他们才可以进行赋值操作。
```

## C++中的注册机制

１、靠着静态类实现。

２、靠着模板偏特化实现。

为什么这些就可以注册呢，因为在代码运行之前会首先初始化静态变量，因为静态变量的初始化是在main函数之前进行的。那么main函数运行的时候就可以访问到静态变量初始化的某些内容了。

有个问题，前面看的代码应该都是静态注册，那pytorch中的动态注册是怎么实现？

在build/aten/src/ATen/CPUType.cpp文件中我们看到了TORCH_LIBRARY_IMPL(aten, CPU, m) 这么个函数，这个是在干嘛？



## 追踪一下torch.add算子的调用栈

```bash
#0  at::native::(anonymous namespace)::add_kernel (iter=..., alpha_scalar=...) at aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.DEFAULT.cpp:24
#1  0x00007fffe7d0629d in at::native::DispatchStub<void (*)(at::TensorIterator&, c10::Scalar), at::native::add_stub>::operator()<at::TensorIterator&, c10::Scalar&> (this=0x7fffee26f7c0 <at::native::add_stub>, device_type=c10::DeviceType::CPU)
    at ../aten/src/ATen/native/DispatchStub.h:79
#2  0x00007fffe7cf7625 in at::native::add (self=..., other=..., alpha=...) at ../aten/src/ATen/native/BinaryOps.cpp:64
#3  0x00007fffe8325653 in at::CPUType::add_Tensor (self=..., other=..., alpha=...) at aten/src/ATen/CPUType.cpp:49
#4  0x00007fffe7ad923f in c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, c10::Scalar), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, c10::Scalar> >::operator() (
    this=0x555555cd6030, args#0=..., args#1=..., args#2=...) at ../aten/src/ATen/core/boxing/impl/WrapFunctionIntoRuntimeFunctor.h:18
#5  0x00007fffe7ae0b15 in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, c10::Scalar), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, c10::Scalar> >, at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, c10::Scalar) (functor=0x555555cd6030, args#0=..., args#1=..., args#2=...)
    at ../aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:416
#6  0x00007fffe861cbd3 in c10::callUnboxedKernelFunction<at::Tensor, at::Tensor const&, at::Tensor const&, c10::Scalar>(void*, c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, c10::Scalar&&) (
    unboxed_kernel_func=0x7fffe7ae0a9b <c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, c10::Scalar), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, c10::Scalar> >, at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, c10::Scalar)>, functor=0x555555cd6030) at ../aten/src/ATen/core/boxing/KernelFunction_impl.h:43
#7  0x00007fffe859b6e8 in c10::KernelFunction::call<at::Tensor, at::Tensor const&, at::Tensor const&, c10::Scalar> (this=0x555555a5d5b0, opHandle=...) at ../aten/src/ATen/core/boxing/KernelFunction_impl.h:53
#8  0x00007fffe8514f37 in c10::Dispatcher::callWithDispatchKey<at::Tensor, at::Tensor const&, at::Tensor const&, c10::Scalar>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)> const&, c10::DispatchKey, at::Tensor const&, at::Tensor const&, c10::Scalar) const (this=0x7fffee26e220 <c10::Dispatcher::singleton()::_singleton>, op=..., dispatchKey=c10::DispatchKey::CPU) at ../aten/src/ATen/core/dispatch/Dispatcher.h:370
#9  0x00007fffe848930d in c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&, c10::Scalar>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)> const&, at::Tensor const&, at::Tensor const&, c10::Scalar) const (this=0x7fffee26e220 <c10::Dispatcher::singleton()::_singleton>, op=...) at ../aten/src/ATen/core/dispatch/Dispatcher.h:381
#10 0x00007fffe845bbe9 in c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)>::call(at::Tensor const&, at::Tensor const&, c10::Scalar) const (
    this=0x7fffee276390 <at::add(at::Tensor const&, at::Tensor const&, c10::Scalar)::op>, args#0=..., args#1=..., args#2=...) at ../aten/src/ATen/core/dispatch/Dispatcher.h:323
#11 0x00007fffe83e9f31 in at::add (self=..., other=..., alpha=...) at aten/src/ATen/Functions.cpp:806
#12 0x00007fffe98c6b0d in torch::autograd::VariableType::(anonymous namespace)::<lambda()>::operator()(void) const (__closure=0x7fffffffc9f0) at ../torch/csrc/autograd/generated/VariableType_2.cpp:828
#13 0x00007fffe98c6ec1 in torch::autograd::VariableType::(anonymous namespace)::add_Tensor (self=..., other=..., alpha=...) at ../torch/csrc/autograd/generated/VariableType_2.cpp:829
#14 0x00007fffe7ad923f in c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, c10::Scalar), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, c10::Scalar> >::operator() (
    this=0x555555dc7dc0, args#0=..., args#1=..., args#2=...) at ../aten/src/ATen/core/boxing/impl/WrapFunctionIntoRuntimeFunctor.h:18
#15 0x00007fffe7ae0b15 in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, c10::Scalar), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, c10::Scalar> >, at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, c10::Scalar) (functor=0x555555dc7dc0, args#0=..., args#1=..., args#2=...)
    at ../aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:416
#16 0x00007fffe861cbd3 in c10::callUnboxedKernelFunction<at::Tensor, at::Tensor const&, at::Tensor const&, c10::Scalar>(void*, c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, c10::Scalar&&) (
    unboxed_kernel_func=0x7fffe7ae0a9b <c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor (*)(at::Tensor const&, at::Tensor const&, c10::Scalar), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, c10::Scalar> >, at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)>::call(c10::OperatorKernel*, at::Tensor const&, at::Tensor const&, c10::Scalar)>, functor=0x555555dc7dc0) at ../aten/src/ATen/core/boxing/KernelFunction_impl.h:43
#17 0x00007fffe859b6e8 in c10::KernelFunction::call<at::Tensor, at::Tensor const&, at::Tensor const&, c10::Scalar> (this=0x555555a5d8f0, opHandle=...) at ../aten/src/ATen/core/boxing/KernelFunction_impl.h:53
#18 0x00007fffe8514f37 in c10::Dispatcher::callWithDispatchKey<at::Tensor, at::Tensor const&, at::Tensor const&, c10::Scalar>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)> const&, c10::DispatchKey, at::Tensor const&, at::Tensor const&, c10::Scalar) const (this=0x7fffee26e220 <c10::Dispatcher::singleton()::_singleton>, op=..., dispatchKey=c10::DispatchKey::Autograd) at ../aten/src/ATen/core/dispatch/Dispatcher.h:370
#19 0x00007fffe848930d in c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&, c10::Scalar>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)> const&, at::Tensor const&, at::Tensor const&, c10::Scalar) const (this=0x7fffee26e220 <c10::Dispatcher::singleton()::_singleton>, op=...) at ../aten/src/ATen/core/dispatch/Dispatcher.h:381
#20 0x00007fffe845bbe9 in c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::Scalar)>::call(at::Tensor const&, at::Tensor const&, c10::Scalar) const (this=0x7fffee27a168 <at::Tensor::add(at::Tensor const&, c10::Scalar) const::op>, 
    args#0=..., args#1=..., args#2=...) at ../aten/src/ATen/core/dispatch/Dispatcher.h:323
#21 0x00007fffe8810ebe in at::Tensor::add (this=0x7fffffffd180, other=..., alpha=...) at aten/src/ATen/core/TensorMethods.cpp:389
#22 0x00007fffeed8a0f6 in torch::autograd::<lambda(const at::Tensor&, const at::Tensor&, c10::Scalar)>::operator()(const at::Tensor &, const at::Tensor &, c10::Scalar) const (__closure=0x7fffffffd2c0, self=..., other=..., alpha=...)
    at ../torch/csrc/autograd/generated/python_torch_functions.cpp:4712
#23 0x00007fffeed8a71b in torch::autograd::THPVariable_add (self_=0x0, args=0x7ffff6a2dac0, kwargs=0x0) at ../torch/csrc/autograd/generated/python_torch_functions.cpp:4714
```



关于算子的动态分发：

这里面USE_STATIC_DISPATCH指的就是静态分发的意思主要是在移动端设备上进行分发，移动端设备上会静态的分发到CPU上进行计算，但是一般在PC机器上都是动态分发，所以重点研究动态分发机制。

build/aten/src/ATen/Functions.cpp

```c++
Tensor add(const Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef USE_STATIC_DISPATCH
    at::AutoNonVariableTypeMode _var_guard(true);
    DispatchKeySet _dk_set = c10::detail::multi_dispatch_key_set(self, other);
    DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
    DispatchKey _dk = c10::impl::dispatchTypeId(_dk_set, _dk_mask);
    switch (dispatchKeyToBackend(_dk)) {
        case Backend::CPU:
            return CPUType::add_Tensor(self, other, alpha);
            break;
        default:
            AT_ERROR("add not implemented for ", at::toString(_dk));
    }
#else
    static auto op = c10::Dispatcher::singleton()　// 这部分代码是动态分发的
        .findSchemaOrThrow("aten::add", "Tensor")
        .typed<Tensor (const Tensor &, const Tensor &, Scalar)>();
    return op.call(self, other, alpha);
#endif
}
```

pytorch中和动态分发相关的代码都是在aten/src/ATen/core/dispatch目录下

这里不是很清楚findOp这一块是啥个意思。这里会涉及到两个数据结构，一个是LeftRight，另外一个是ska::flat_hash_map。我们到c10目录下看看到底是个啥？

```c++
c10::optional<OperatorHandle> Dispatcher::findOp(const OperatorName& overload_name) {
  return operatorLookupTable_.read([&] (const ska::flat_hash_map<OperatorName, OperatorHandle>& operatorLookupTable) -> c10::optional<OperatorHandle> {
    auto found = operatorLookupTable.find(overload_name);
    if (found == operatorLookupTable.end()) {
      return c10::nullopt;
    }
    return found->second;
  });
}

c10::optional<OperatorHandle> Dispatcher::findSchema(const OperatorName& overload_name) {
  auto it = findOp(overload_name);
  if (it.has_value()) {
    if (it->hasSchema()) {
      return it;
    } else {
      return c10::nullopt;
    }
  } else {
    return it;
  }
}

OperatorHandle Dispatcher::findSchemaOrThrow(const char* name, const char* overload_name) {
  auto it = findSchema({name, overload_name});
  if (!it.has_value()) {
    // Check if we have ANYTHING; if that's the case, that means you're
    // missing schema
    auto it2 = findOp({name, overload_name});
    if (!it2.has_value()) {
      TORCH_CHECK(false, "Could not find schema for ", name, ".", overload_name);
    } else {
      TORCH_CHECK(false, "Could not find schema for ", name, ".", overload_name,
        " but we found an implementation; did you forget to def() the operator?");
    }
  }
  return it.value();
}
```

